// -----------------------------------------------------------------------
// <copyright file="SampleGenerator.cs" company="No company">
//  Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,
//          G. Louppe, S. Zyuzin
//  License: BSD 3 clause
// </copyright>
// -----------------------------------------------------------------------

namespace Sharpkit.Learn.Datasets
{
    using System;
    using System.Collections.Generic;
    using System.Linq;
    using MathNet.Numerics.Distributions;
    using MathNet.Numerics.LinearAlgebra.Double;
    using MathNet.Numerics.LinearAlgebra.Generic;
    using MathNet.Numerics.LinearAlgebra.Generic.Factorization;

    /// <summary>
    /// Generate samples of synthetic data sets.
    /// </summary>
    public class SampleGenerator
    {
        /// <summary>
        /// Generate a random regression problem.
        /// <para>
        /// The input set can either be well conditioned (by default) or have a low
        /// rank-fat tail singular profile. See <see cref="MakeLowRankMatrix"/> for
        /// more details.
        /// </para>
        /// <para>
        /// The output is generated by applying a (potentially biased) random linear
        /// regression model with `numInformative` nonzero regressors to the previously
        /// generated input and some gaussian centered noise with some adjustable
        /// scale.
        /// </para>
        /// </summary>
        /// <param name="numSamples">The number of samples.</param>
        /// <param name="numFeatures">The number of features.</param>
        /// <param name="numInformative">The number of informative features, i.e., the number of features used
        /// to build the linear model used to generate the output.</param>
        /// <param name="numTargets">The number of regression targets, i.e., the dimension of the y output
        /// vector associated with a sample. By default, the output is a scalar.</param>
        /// <param name="bias">The bias term in the underlying linear model.</param>
        /// <param name="effectiveRank">if not null:
        ///   The approximate number of singular vectors required to explain most
        ///   of the input data by linear combinations. Using this kind of
        ///   singular spectrum in the input allows the generator to reproduce
        ///   the correlations often observed in practice.
        ///   if null:
        ///       The input set is well conditioned, centered and gaussian with
        ///       unit variance.</param>
        /// <param name="tailStrength">Value between 0.0 and 1.0.
        /// The relative importance of the fat noisy tail of the singular values
        /// profile if <paramref name="effectiveRank"/> is not None.</param>
        /// <param name="noise">The standard deviation of the gaussian noise applied to the output.</param>
        /// <param name="shuffle">Shuffle the samples and the features.</param>
        /// <param name="coef">If <c>true</c>, the coefficients of the underlying linear model are returned.</param>
        /// <param name="random">Instance of <see cref="Random"/>.</param>
        /// <returns>Instance of <see cref="RegressionResult"/>.</returns>
        public static RegressionResult MakeRegression(
            int numSamples = 100,
            int numFeatures = 100,
            int numInformative = 10,
            int numTargets = 1,
            double bias = 0.0,
            int? effectiveRank = null,
            double tailStrength = 0.5,
            double noise = 0.0,
            bool shuffle = true,
            bool coef = false,
            Random random = null)
        {
            var generator = random ?? new Random();

            Matrix<double> x;
            Matrix<double> y;
            if (effectiveRank == null)
            {
                // Randomly generate a well conditioned input set
                x = DenseMatrix.CreateRandom(
                    numSamples,
                    numFeatures,
                    new ContinuousUniform { RandomSource = generator });
            }
            else
            {
                // Randomly generate a low rank, fat tail input set
                x = MakeLowRankMatrix(
                    numSamples,
                    numFeatures,
                    effectiveRank.Value,
                    tailStrength,
                    generator);
            }
    
            // Generate a ground truth model with only 'numInformative' features being non
            // zeros (the other features are not correlated to y and should be ignored
            // by a sparsifying regularizers such as L1 or elastic net)
            Matrix groundTruth = DenseMatrix.CreateRandom(numFeatures, numTargets, new ContinuousUniform(0, 100));
            groundTruth.MapIndexedInplace((i, j, v) => i >= numInformative ? 0 : v);

            y = x * groundTruth;
            y.MapInplace(v => v + bias);

            // Add noise
            if (noise > 0.0)
            {
                var normalDistribution = new Normal { RandomSource = generator };
                DenseMatrix randomMatrix = DenseMatrix.CreateRandom(y.RowCount, y.ColumnCount, normalDistribution);
                y = y + (randomMatrix * noise);
            }

            // Randomly permute samples and features
            if (shuffle)
            {
                // todo:
                throw new ArgumentException("Shuffle is not yet supported");
            }

            if (coef)
            {
                return new RegressionResult { X = x, Y = y, Coef = groundTruth };
            }

            return new RegressionResult { X = x, Y = y };
        }

        /// <summary>
        /// Generate a mostly low rank matrix with bell-shaped singular values
        /// <para>
        /// Most of the variance can be explained by a bell-shaped curve of width
        /// effective_rank: the low rank part of the singular values profile is:
        /// </para>
        /// <para>
        /// (1 - tailStrength) * exp(-1.0 * (i / effectiveRank) ** 2)
        /// </para>
        /// <para>
        /// The remaining singular values' tail is fat, decreasing as:
        /// </para>
        /// <para>
        /// tailStrength * exp(-0.1 * i / effectiveRank).
        /// </para>
        /// <para>
        /// The low rank part of the profile can be considered the structured
        /// signal part of the data while the tail can be considered the noisy
        /// part of the data that cannot be summarized by a low number of linear
        /// components (singular vectors).
        /// </para>
        /// <para>
        /// This kind of singular profiles is often seen in practice, for instance:
        ///   - gray level pictures of faces
        ///   - TF-IDF vectors of text documents crawled from the web
        /// </para>
        /// </summary>
        /// <param name="numSamples">The number of samples.</param>
        /// <param name="numFeatures">The number of features.</param>
        /// <param name="effectiveRank">The approximate number of singular vectors required to explain most of
        /// the data by linear combinations.</param>
        /// <param name="tailStrength">The relative importance of the fat noisy tail of the singular values
        /// profile.</param>
        /// <param name="randomState">Instance of <see cref="Random"/>.</param>
        /// <returns>The matrix.</returns>
        public static Matrix<double> MakeLowRankMatrix(
            int numSamples = 100,
            int numFeatures = 100,
            int effectiveRank = 10,
            double tailStrength = 0.5,
            Random randomState = null)
        {
            Random generator = randomState ?? new Random();
            int n = Math.Min(numSamples, numFeatures);

            // Random (ortho normal) vectors
            var normalDistribution = new Normal { RandomSource = generator };
            var u = DenseMatrix.CreateRandom(numSamples, n, normalDistribution).QR(QRMethod.Thin).Q;
            var v = DenseMatrix.CreateRandom(numFeatures, n, normalDistribution).QR(QRMethod.Thin).Q;

            // Index of the singular values
            var singularInd = DenseVector.OfEnumerable(Enumerable.Range(0, n).Select(val => (double)val));

            // Build the singular profile by assembling signal and noise components
            Vector vect = singularInd / effectiveRank;
            vect.MapInplace(val => Math.Exp(-val * val));
            var lowRank = (1 - tailStrength) * vect;

            vect = singularInd / effectiveRank;
            vect.MapInplace(val => Math.Exp(-0.1 * val));
            var tail = tailStrength * vect;
            var s = DenseMatrix.Identity(n).MulRowVector(lowRank + tail);

            return u * s * v.Transpose();
        }

        /// <summary>
        /// Generate a random regression problem with sparse uncorrelated design
        /// <para>
        /// This dataset is described in Celeux et al [1]. as::
        /// </para>
        /// <para>
        /// X ~ N(0, 1)
        /// y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]
        /// </para>
        /// <para>
        /// Only the first 4 features are informative. The remaining features are
        /// useless.
        /// </para>
        /// </summary>
        /// <param name="numSamples">The number of samples.</param>
        /// <param name="numFeatures">The number of features.</param>
        /// <param name="random">Instance of <see cref="Random"/>.</param>
        /// <returns>Instance of <see cref="RegressionResult"/> with <see cref="RegressionResult.Coef"/> not populated.</returns>
        /// <remarks>
        ///     References
        ///     ----------
        ///      .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,
        ///       "Regularization in regression: comparing Bayesian and frequentist
        ///        methods in a poorly informative situation", 2009.
        /// </remarks>
        public static RegressionResult MakeSparseUncorrelated(
            int numSamples = 100,
            int numFeatures = 10,
            Random random = null)
        {
            var distribution = new Normal(0, 1) { RandomSource = random };
            Matrix x = DenseMatrix.CreateRandom(numSamples, numFeatures, distribution);

            Vector y = new DenseVector(numSamples);
            for (int i = 0; i < numSamples; i++)
            {
                var mean = x[i, 0] + (2 * x[i, 1]) - (2 * x[i, 2]) - (1.5 * x[i, 3]);
                y[i] = new Normal(mean, 1.0) { RandomSource = random }.Sample();
            }

            return new RegressionResult { X = x, Y = MatrixExtensions.ToColumnMatrix(y) };
        }

        /// <summary>
        /// Generate a random n-class classification problem.
        /// </summary>
        /// <param name="nSamples">The number of samples.</param>
        /// <param name="nFeatures">The total number of features. These comprise <paramref name="nInformative"/>
        /// informative features, <paramref name="nRedundant"/> redundant features, <paramref name="nRepeated"/>
        /// dupplicated features and `<paramref name="nFeatures"/>-<paramref name="nInformative"/>-<paramref name="nRedundant"/>-
        /// <paramref name="nRepeated"/>` useless features drawn at random.</param>
        /// <param name="nInformative">The number of informative features. Each class is composed of a number
        /// of gaussian clusters each located around the vertices of a hypercube
        /// in a subspace of dimension <paramref name="nInformative"/>. For each cluster,
        /// informative features are drawn independently from  N(0, 1) and then
        /// randomly linearly combined in order to add covariance. The clusters
        /// are then placed on the vertices of the hypercube.</param>
        /// <param name="nRedundant">The number of redundant features. These features are generated as
        /// random linear combinations of the informative features.</param>
        /// <param name="nRepeated"> The number of dupplicated features, drawn randomly from the informative
        /// and the redundant features.
        /// </param>
        /// <param name="nClasses">The number of classes (or labels) of the classification problem.</param>
        /// <param name="nClustersPerClass">The number of clusters per class.</param>
        /// <param name="weights">The proportions of samples assigned to each class. If None, then
        /// classes are balanced. Note that if `len(weights) == n_classes - 1`,
        /// then the last class weight is automatically inferred.
        /// </param>
        /// <param name="flipY">The fraction of samples whose class are randomly exchanged.</param>
        /// <param name="classSep">The factor multiplying the hypercube dimension.</param>
        /// <param name="hypercube">If True, the clusters are put on the vertices of a hypercube. If
        /// False, the clusters are put on the vertices of a random polytope.</param>
        /// <param name="shift">Shift all features by the specified value. If None, then features
        /// are shifted by a random value drawn in [-class_sep, class_sep].</param>
        /// <param name="scale">Multiply all features by the specified value. If None, then features
        /// are scaled by a random value drawn in [1, 100]. Note that scaling
        /// happens after shifting.
        /// </param>
        /// <param name="shuffle">Shuffle the samples and the features.</param>
        /// <param name="randomState">Random generator.</param>
        /// <returns>array of shape [n_samples]
        /// The integer labels for class membership of each sample.</returns>
        /// <remarks>
        /// The algorithm is adapted from Guyon [1] and was designed to generate
        /// the "Madelon" dataset.
        /// References
        /// ----------
        /// .. [1] I. Guyon, "Design of experiments for the NIPS 2003 variable
        ///   selection benchmark", 2003.
        /// </remarks>
        public static Classification MakeClassification(
            int nSamples = 100,
            int nFeatures = 20,
            int nInformative = 2,
            int nRedundant = 2,
            int nRepeated = 0,
            int nClasses = 2,
            int nClustersPerClass = 2,
            List<double> weights = null,
            double flipY = 0.01,
            double classSep = 1.0,
            bool hypercube = true,
            double? shift = 0.0,
            double? scale = 1.0,
            bool shuffle = true,
            Random randomState = null)
        {
            var generator = randomState ?? new Random();

            // Count features, clusters and samples
            if (nInformative + nRedundant + nRepeated > nFeatures)
            {
                throw new ArgumentException("Number of informative, redundant and repeated " +
                                            "features must sum to less than the number of total" +
                                            " features");
            }

            if (nInformative * nInformative < nClasses * nClustersPerClass)
            {
                throw new ArgumentException(
                    "n_classes * n_clusters_per_class must" +
                    "be smaller or equal 2 ** n_informative");
            }

            if (weights != null && !new[] { nClasses, nClasses - 1 }.Contains(weights.Count))
            {
                throw new ArgumentException("Weights specified but incompatible with number of classes.");
            }

            int nUseless = nFeatures - nInformative - nRedundant - nRepeated;
            int nClusters = nClasses * nClustersPerClass;

            if (weights != null && weights.Count == nClasses - 1)
            {
                weights.Add(1.0 - weights.Sum());
            }

            if (weights == null) 
            {
                weights = Enumerable.Repeat(1.0 / nClasses, nClasses).ToList();
                weights[weights.Count - 1] = 1.0 - weights.Take(weights.Count - 1).Sum();
            }

            var nSamplesPerCluster = new List<int>();

            for (int k = 0; k < nClusters; k++)
            {
                nSamplesPerCluster.Add(
                    (int)(nSamples * weights[k % nClasses] / nClustersPerClass));
            }

            for (int i = 0; i < nSamples - nSamplesPerCluster.Sum(); i++)
            {
                nSamplesPerCluster[i % nClusters] += 1;
            }

            // Intialize X and y
            Matrix x = new DenseMatrix(nSamples, nFeatures);
            int[] y = new int[nSamples];

            // Build the polytope
            Matrix c = new DenseMatrix(1 << nInformative, nInformative);
            for (int i = 0; i < 1 << nInformative; i++)
            {
                var row = new DenseVector(nInformative);
                for (int bitN = 0; bitN < nInformative; bitN++)
                {
                    row[bitN] = (i & (1 << bitN)) == 1 ? classSep : -classSep;
                }

                c.SetRow(i, row);
            }

            if (!hypercube)
            {
                for (int k = 0; k < nClusters; k++)
                {
                    c.SetRow(k, c.Row(k) * generator.NextDouble());
                }

                for (int f = 0; f < nInformative; f++)
                {
                    c.SetColumn(f, c.Column(f) * generator.NextDouble());
                }
            }

            // todo:
            // generator.shuffle(C)

            // Loop over all clusters
            int pos = 0;
            int posEnd = 0;

            for (int k = 0; k < nClusters; k++)
            {
                // Number of samples in cluster k
                int nSamplesK = nSamplesPerCluster[k];

                // Define the range of samples
                pos = posEnd;
                posEnd = pos + nSamplesK;

                // Assign labels
                for (int l = pos; l < posEnd; l++)
                {
                    y[l] = k % nClasses;
                }

                // Draw features at random
                var subMatrix = DenseMatrix.CreateRandom(
                    nSamplesK,
                    nInformative,
                    new Normal { RandomSource = generator });

                x.SetSubMatrix(pos, nSamplesK, 0, nInformative, subMatrix);

                // Multiply by a random matrix to create co-variance of the features
                var uniform = new ContinuousUniform(-1, 1) { RandomSource = generator };
                Matrix a = DenseMatrix.CreateRandom(nInformative, nInformative, uniform);

                x.SetSubMatrix(
                    pos,
                    nSamplesK,
                    0,
                    nInformative,
                    x.SubMatrix(pos, nSamplesK, 0, nInformative) * a);

                // Shift the cluster to a vertice
                var v = x.SubMatrix(pos, nSamplesK, 0, nInformative).AddRowVector(c.Row(k));
                x.SetSubMatrix(pos, nSamplesK, 0, nInformative, v);
            }

            // Create redundant features
            if (nRedundant > 0)
            {
                var uniform = new ContinuousUniform(-1, 1) { RandomSource = generator };
                Matrix b = DenseMatrix.CreateRandom(nInformative, nRedundant, uniform);
                x.SetSubMatrix(
                    0,
                    x.RowCount,
                    nInformative,
                    nRedundant,
                    x.SubMatrix(0, x.RowCount, 0, nInformative) * b);
            }

            // Repeat some features
            if (nRepeated > 0)
            {
                int n = nInformative + nRedundant;
                for (int i = 0; i < nRepeated; i++)
                {
                    int r = (int)((generator.Next(nRepeated) * (n - 1)) + 0.5);
                    x.SetColumn(i, x.Column(r));
                }
            }

            // Fill useless features
            var denseMatrix = DenseMatrix.CreateRandom(nSamples, nUseless, new Normal { RandomSource = generator });
            x.SetSubMatrix(0, nSamples, nFeatures - nUseless, nUseless, denseMatrix);

            // Randomly flip labels
            if (flipY >= 0.0)
            {
                for (int i = 0; i < nSamples; i++)
                {
                    if (generator.NextDouble() < flipY)
                    {
                        y[i] = generator.Next(nClasses);
                    }
                }
            }

            // Randomly shift and scale
            bool constantShift = shift != null;
            bool constantScale = scale != null;

            for (int f = 0; f < nFeatures; f++)
            {
                if (!constantShift)
                {
                    shift = ((2 * generator.NextDouble()) - 1) * classSep;
                }

                if (!constantScale)
                {
                    scale = 1 + (100 * generator.NextDouble());
                }

                x.SetColumn(f, (x.Column(f) + shift.Value) * scale.Value);
            }

            // Randomly permute samples and features
            // todo:
            /*
            if (shuffle)
            {
                X, y = util_shuffle(X, y, random_state=generator)

                indices = np.arange(n_features)
                generator.shuffle(indices)
                X[:, :] = X[:, indices]
            }*/

            return new Classification { X = x, Y = y };
        }

        /// <summary>
        /// Regression result.
        /// </summary>
        public class RegressionResult
        {
            /// <summary>
            /// Initializes a new instance of the RegressionResult class.
            /// </summary>
            internal RegressionResult()
            {
            }

            /// <summary>
            /// Gets the input samples.
            /// </summary>
            public Matrix<double> X { get; internal set; }

            /// <summary>
            /// Gets the output values.
            /// </summary>
            public Matrix<double> Y { get; internal set; }

            /// <summary>
            /// Gets the coefficient of the underlying linear model.
            /// </summary>
            public Matrix<double> Coef { get; internal set; }
        }

        /// <summary>
        /// Result of <see cref="MakeClassification"/>.
        /// </summary>
        public class Classification
        {
            /// <summary>
            /// Gets or sets the generated samples.
            /// Matrix of shape [n_samples, n_features]
            /// </summary>
            public Matrix X { get; set; }

            /// <summary>
            /// Gets or sets target lables.
            /// </summary>
            public int[] Y { get; set; }
        }
    }
}
